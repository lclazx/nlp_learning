{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract_data_with_bert_hub.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lclazx/nlp_learning/blob/master/extract_data_with_bert_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_TTthQGi2JC",
        "colab_type": "text"
      },
      "source": [
        "# 步骤一：引用所需资源"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBVMv99nipRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "440372d2-3913-49c6-dbea-f2632bf5cf27"
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arPAta2SjfvG",
        "colab_type": "text"
      },
      "source": [
        "步骤二：准备数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZZS48B_jjwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 token_label_ids,\n",
        "                 predicate_label_id,\n",
        "                 is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.token_label_ids = token_label_ids\n",
        "        self.predicate_label_id = predicate_label_id\n",
        "        self.is_real_example = is_real_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG-fN6RTzYcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJz2cbSm-2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, token_label_list, predicate_label_list, max_seq_length, tokenizer):\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0]*max_seq_length,\n",
        "        input_mask=[0]*max_seq_length,\n",
        "        segment_ids=[0]*max_seq_length,\n",
        "        token_label_ids=[0]*max_seq_length,\n",
        "        predicate_label_id=[0],\n",
        "        is_real_example=False\n",
        "    )\n",
        "\n",
        "  token_label_map={}\n",
        "  for(i, label) in enumerate(token_label_list):\n",
        "    token_label_map[label]=i\n",
        "  predicate_label_map={}\n",
        "  for(i, label) in enumerate(predicate_label_list):\n",
        "    predicate_label_map[label] = i\n",
        "  \n",
        "  # text_token = example.text_token.split['\\t'][0].split(' ')\n",
        "  # if example.token_label is not None:\n",
        "  #   token_label = example.token_label.split('\\t')[0].split(' ')\n",
        "  # else:\n",
        "  #   token_label = [\"O\"]*len(text_token)\n",
        "  # assert len(text_token) == len(token_label)\n",
        "\n",
        "  # text_predicate = example.text_token.split('\\t')[1]\n",
        "  # if example.token_label is not None:\n",
        "  #   token_predicate = example.token_label.split('\\t')[1]\n",
        "  # else:\n",
        "  #   token_predicate = text_predicate\n",
        "  text_token = example.text_token\n",
        "  token_label = text_token.pop()\n",
        "  text_predicate = example.token_label\n",
        "  token_predicate = text_predicate.pop()\n",
        "  assert token_predicate == text_predicate\n",
        "\n",
        "  tokens_b = [text_predicate]*len(text_token)\n",
        "  predicate_id = predicate_label_map[text_predicate]\n",
        "\n",
        "  _truncate_seq_pair(text_token, tokens_b, max_seq_length-3)\n",
        "\n",
        "  tokens = []\n",
        "  token_label_ids = []\n",
        "  segment_ids = []\n",
        "  tokens.append('[CLS]')\n",
        "  token_label_ids.append(token_label_map[\"[CLS]\"])\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  for token, label in zip(text_token, token_label):\n",
        "    tokens.append(text_token)\n",
        "    token_label_ids.append(token_label)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  tokens.append('[SEP]')\n",
        "  token_label_ids.append(token_label_map['[SEP]'])\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  bias = 1\n",
        "  for token in tokens_b:\n",
        "    input_ids.append(predicate_id + bias)\n",
        "    segment_ids.append(1)\n",
        "    token_label_ids.append(token_label_map['[category]'])\n",
        "\n",
        "  input_ids.append(tokenizer.convert_tokens_to_ids(['[SEP]'])[0])\n",
        "  segment_ids.append(1)\n",
        "  token_label_ids.append(token_label_map('[SEP]'))\n",
        "\n",
        "  input_mask = [1]*len(input_ids)\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "    token_label_ids.append(0)\n",
        "    tokens.append('[Padding]')\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "  assert len(token_label_ids) == max_seq_length\n",
        "\n",
        "  if ex_index<5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid:%s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % ''.join([tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]))\n",
        "    tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % ' '.join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info('token_label_ids: %s' % ' '.join([str(x) for x in token_label_ids]))\n",
        "    tf.logging.info('predicate_id: %s' % str(predicate_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids = input_ids,\n",
        "      input_mask = input_mask,\n",
        "      segment_ids = segment_ids,\n",
        "      token_label_ids = token_label_ids,\n",
        "      predicate_label_id = [predicate_id],\n",
        "      is_real_example=True\n",
        "  )\n",
        "\n",
        "  return feature\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qaRENKY_uZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "  def __init__(self, guid, text_token, token_labels):\n",
        "    self.guid=guid\n",
        "    self.text_token=text_token\n",
        "    self.token_label=token_labels\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZPVRVIQAkwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "OUTPUT_DIR = 'extract_training_output' #@param\n",
        "DO_DELETE =  False#@param\n",
        "USE_BUCKET = True #@param\n",
        "BUCKET = 'bert_classification'#@param\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  \n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rj_hoVGC6YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from apiclient.http import MediaIoBaseDownload\n",
        "def download_file(output_dir, source_file):\n",
        "  with open(output_dir, 'wb') as f:\n",
        "    request = gcs_service.objects().get_media(bucket=BUCKET, object=source_file)\n",
        "    media = MediaIoBaseDownload(f, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "      _, done = media.next_chunk()\n",
        "\n",
        "download_file(output_dir='/tmp/train_data.json', source_file='raw_data/train_data.json/train_data.json')\n",
        "download_file(output_dir='/tmp/test_data.json', source_file='raw_data/test_data_postag.json/test_data_postag.json')\n",
        "download_file(output_dir='/tmp/dev_data.json', source_file='raw_data/dev_data.json/dev_data.json')\n",
        "\n",
        "print('Downloaded')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zWVyah7YBYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4ddaa695-180e-4a46-fba1-b600e8ebb217"
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1'\n",
        "TRAINABLE = True\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature='tokenization_info', as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info['vocab_file'], tokenization_info['do_lower_case']])\n",
        "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycCACm7uRKaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_bio_in_text_token(so_object_text, text_token):\n",
        "  for i, token in enumerate(text_token):\n",
        "    trimmed_token = token.replace('##', '')\n",
        "    if so_object_text.startswith(trimmed_token):\n",
        "      for j in range(i+1,len(text_token)):\n",
        "        concatenated_token = ''.join([x.replace(\"##\",'') for x in text_token[i:j]])\n",
        "        if so_object_text == concatenated_token:\n",
        "          return i, j\n",
        "        elif so_object_text.startswith(concatenated_token):\n",
        "          continue\n",
        "        else:\n",
        "          break\n",
        "  return -1,-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8edyypgEpD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_SPO_Example(data):\n",
        "  text = data['text']\n",
        "  spo_list = data['spo_list']\n",
        "  text_token = tokenizer.tokenize(text)\n",
        "  examples = []\n",
        "  for spo in spo_list:\n",
        "    subject = spo['subject']\n",
        "    obj = spo['object']\n",
        "    predicate = spo['predicate']\n",
        "    subject_start_index, subject_end_index = find_bio_in_text_token(subject, text_token)\n",
        "    obj_start_index, obj_end_index = find_bio_in_text_token(obj, text_token)\n",
        "    text_token_with_prediction=[]\n",
        "    for token in text_token:\n",
        "      text_token_with_prediction.append(token)\n",
        "    token_labels = ['O']*len(text_token_with_prediction)\n",
        "    text_token_with_prediction.append(predicate)\n",
        "    token_labels.append(predicate)\n",
        "\n",
        "    if subject_start_index>-1:\n",
        "      token_labels[subject_start_index] = 'B-SUB'\n",
        "      for i in range(subject_start_index+1, subject_end_index):\n",
        "        token_labels[i] = 'I-SUB'\n",
        "    \n",
        "    if obj_start_index > -1:\n",
        "      token_labels[obj_start_index] = 'B-OBJ'\n",
        "      for i in range(obj_start_index+1, obj_end_index):\n",
        "        token_labels[i] = 'I-OBJ'\n",
        "\n",
        "    #examples.append({'text_token': text_token_with_prediction, 'token_labels': token_labels})\n",
        "    example = InputExample(guid=None, text_token=text_token_with_prediction, token_labels=token_labels)\n",
        "    examples.append(example)\n",
        "  return examples\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfHAZBFQbgSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "304ab9d0-cd93-4db1-b3f5-c5a649fe2427"
      },
      "source": [
        "test_data = {'text': '查尔斯·阿兰基斯（Charles Aránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部',\n",
        "             'spo_list':[\n",
        "                       {\n",
        "                          'predicate':'出生地',\n",
        "                          'object':'圣地亚哥',\n",
        "                          'subject':'查尔斯·阿兰基斯'\n",
        "                       },\n",
        "                       {\n",
        "                          'predicate':'出生日期',\n",
        "                          'object':'1989年4月17日',\n",
        "                          'subject':'查尔斯·阿兰基斯'\n",
        "\n",
        "                       }\n",
        "                  ]\n",
        "        }\n",
        "\n",
        "examples = build_SPO_Example(test_data)\n",
        "examples"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.InputExample at 0x7f92a6558438>,\n",
              " <__main__.InputExample at 0x7f92a65584a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO9hMMDtf4TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predicate_labels():\n",
        "  return ['丈夫', '上映时间', '专业代码', '主持人', '主演', '主角', '人口数量', '作曲', '作者', '作词', '修业年限', '出品公司', '出版社', '出生地', '出生日期', '创始人', '制片人', '占地面积', '号', '嘉宾', '国籍', '妻子', '字', '官方语言', '导演', '总部地点', '成立日期', '所在城市', '所属专辑', '改编自', '朝代', '歌手', '母亲', '毕业院校', '民族', '气候', '注册资本', '海拔', '父亲', '目', '祖籍', '简称', '编剧', '董事长', '身高', '连载网站', '邮政编码', '面积', '首都']\n",
        "def get_token_labels():\n",
        "  BIO_token_labels = [\"[Padding]\", \"[category]\", \"[##WordPiece]\", \"[CLS]\", \"[SEP]\", \"B-SUB\", \"I-SUB\", \"B-OBJ\", \"I-OBJ\", \"O\"]  #id 0 --> [Paddding]\n",
        "  return BIO_token_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmQe8MmYpZ8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "286b223a-89b7-4394-a390-bbc872d62840"
      },
      "source": [
        "examples[0].text_token"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['查',\n",
              " '尔',\n",
              " '斯',\n",
              " '·',\n",
              " '阿',\n",
              " '兰',\n",
              " '基',\n",
              " '斯',\n",
              " '（',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '）',\n",
              " '，',\n",
              " '1989',\n",
              " '年',\n",
              " '4',\n",
              " '月',\n",
              " '17',\n",
              " '日',\n",
              " '出',\n",
              " '生',\n",
              " '于',\n",
              " '智',\n",
              " '利',\n",
              " '圣',\n",
              " '地',\n",
              " '亚',\n",
              " '哥',\n",
              " '，',\n",
              " '智',\n",
              " '利',\n",
              " '职',\n",
              " '业',\n",
              " '足',\n",
              " '球',\n",
              " '运',\n",
              " '动',\n",
              " '员',\n",
              " '，',\n",
              " '司',\n",
              " '职',\n",
              " '中',\n",
              " '场',\n",
              " '，',\n",
              " '效',\n",
              " '力',\n",
              " '于',\n",
              " '德',\n",
              " '国',\n",
              " '足',\n",
              " '球',\n",
              " '甲',\n",
              " '级',\n",
              " '联',\n",
              " '赛',\n",
              " '勒',\n",
              " '沃',\n",
              " '库',\n",
              " '森',\n",
              " '足',\n",
              " '球',\n",
              " '俱',\n",
              " '乐',\n",
              " '部']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhBSvvJBkw7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "36f20d86-3586-44f1-c602-1bcef09a9b5f"
      },
      "source": [
        "convert_single_example(0, examples[0], get_token_labels(), get_predicate_labels(), 256, tokenizer)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4255774d71b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_single_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_token_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_predicate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-dbc64a122599>\u001b[0m in \u001b[0;36mconvert_single_example\u001b[0;34m(ex_index, example, token_label_list, predicate_label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mtext_predicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mtoken_predicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_predicate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mtoken_predicate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext_predicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mtokens_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_predicate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}